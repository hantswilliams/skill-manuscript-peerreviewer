# Review Checklists

## Simple-First Statistical Adequacy Checks

- Are denominators and units of analysis explicitly defined?
- Are inclusion/exclusion rules and cohort construction clear?
- Are missing data amounts reported and handling described?
- Are repeated measures/clustering/non-independence handled?
- Is the primary endpoint clearly defined?
- Are effect sizes and uncertainty intervals reported?
- Are claims based only on p-values instead of magnitude/precision?
- Are multiple comparisons/subgroups acknowledged and handled?
- Would a simpler analysis better answer the stated question?

## Internal Consistency Audit Checklist

- Abstract N matches Methods/Results/Table 1.
- Subgroup totals sum to overall N.
- Percentages match numerators/denominators.
- Inclusion/exclusion flow counts reconcile.
- Effect estimates match text/tables/figures.
- p-values match claims of significance/non-significance.
- CIs match effect direction and interpretation.
- Time windows and follow-up periods are consistent.
- Units/scales are consistent across sections.
- Model/cohort names are identical across methods/results/figures.

## High-Priority Red Flags

- Causal language from observational/cross-sectional design
- Selection bias or immortal time bias risk not addressed
- Confounding inadequately handled
- Target/data/temporal leakage in ML studies
- Overfitting risk with weak validation
- Broad claims without external validation
- Multiplicity or subgroup fishing without prespecification
- Negative findings interpreted as equivalence

## Reporting & Transparency Checks

- Inclusion/exclusion criteria
- Exposure/outcome definitions
- Timing of measurements
- Attrition/cohort construction details
- Feature definitions/preprocessing (predictive models)
- Validation strategy and thresholds
- Software/tools/versions (when relevant)
- Data/code availability statement
- Ethics/IRB/consent statement (human subjects)
- Limitations aligned with actual design

